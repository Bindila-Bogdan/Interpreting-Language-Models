{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "363a078c-6808-4bf5-ad66-750c2047b5b4",
   "metadata": {
    "tags": [],
    "user_expressions": []
   },
   "source": [
    "### 0. Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "62614185-9ef7-461f-a03f-3722799697a9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# These two commands are needed\n",
    "# !pip install transformers\n",
    "# !pip install accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7336fa36-a5dc-43d9-976e-427a9e8c4e9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "1a29510a-b6f4-492a-b028-0776461b1199",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import re\n",
    "import json\n",
    "import spacy\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "import transformers\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import seaborn as sns\n",
    "from functools import reduce\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# import local scripts\n",
    "from data_loader import DatasetLoader\n",
    "from alignment_metrics import *\n",
    "from gpt2_model import GPT2Model\n",
    "\n",
    "# produce repeatable results\n",
    "np.random.seed(seed=42)\n",
    "transformers.set_seed(42)\n",
    "\n",
    "# enable CUDNN deterministic mode\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c4173ef-0513-4bb0-a170-2129d03d67e2",
   "metadata": {
    "tags": [],
    "user_expressions": []
   },
   "source": [
    "### 1. Load the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3b8d313-2265-4d12-8d46-b8f961cbc7e7",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "Below are the names of the datasets used by the authors to check if contrastive explanations identify linguistically appropriate evidence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8bb67dff-7496-415c-b1b2-978768144041",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "anaphor_agreement_datasets = [\n",
    "    \"anaphor_gender_agreement\",\n",
    "    \"anaphor_number_agreement\",\n",
    "]\n",
    "\n",
    "# not sure about the one below\n",
    "\n",
    "argument_structure_datasets = [\"animate_subject_passive\"]\n",
    "\n",
    "determiner_noun_agreement_datasets = [\n",
    "    \"determiner_noun_agreement_1\",\n",
    "    \"determiner_noun_agreement_irregular_1\",\n",
    "    \"determiner_noun_agreement_with_adjective_1\",\n",
    "    \"determiner_noun_agreement_with_adj_irregular_1\",\n",
    "]\n",
    "\n",
    "npi_licesing_datasets = [\n",
    "    \"npi_present_1\",\n",
    "]\n",
    "\n",
    "subject_verb_agreement_datasets = [\n",
    "    \"distractor_agreement_relational_noun\",\n",
    "    \"irregular_plural_subject_verb_agreement_1\",\n",
    "    \"regular_plural_subject_verb_agreement_1\",\n",
    "]\n",
    "\n",
    "phenomena = [\n",
    "    #\"anaphor agreement\",\n",
    "    \"argument structure\",\n",
    "    \"determiner noun agreement\",\n",
    "    \"npi licensing\",\n",
    "    \"subject verb agreement\",\n",
    "]\n",
    "datasets = [\n",
    "    #anaphor_agreement_datasets,\n",
    "    argument_structure_datasets,\n",
    "    determiner_noun_agreement_datasets,\n",
    "    npi_licesing_datasets,\n",
    "    subject_verb_agreement_datasets,\n",
    "]\n",
    "phenomenon_index = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "89280074-a98f-410b-9bb6-db1882ec86da",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_data(datasets):\n",
    "    data_loader = DatasetLoader()\n",
    "\n",
    "    data = (\n",
    "        pd.concat(\n",
    "            [\n",
    "                pd.DataFrame(data_loader.load_data(dataset_name)[\"train\"])[\n",
    "                    [\"sentence_good\", \"sentence_bad\"]\n",
    "                ]\n",
    "                for dataset_name in datasets\n",
    "            ]\n",
    "        )\n",
    "        .reset_index()\n",
    "        .drop(\"index\", axis=1)\n",
    "    )\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bbb4353c-15dd-44cf-8143-0316945e87be",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence_good</th>\n",
       "      <th>sentence_bad</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Raymond is selling this sketch.</td>\n",
       "      <td>Raymond is selling this sketches.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Craig explored that grocery store.</td>\n",
       "      <td>Craig explored that grocery stores.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Eva has scared these children.</td>\n",
       "      <td>Eva has scared these child.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Marcus would conceal that pamphlet.</td>\n",
       "      <td>Marcus would conceal that pamphlets.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Carmen hadn't shocked these customers.</td>\n",
       "      <td>Carmen hadn't shocked these customer.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3995</th>\n",
       "      <td>Sherry wasn't insulting these excited women.</td>\n",
       "      <td>Sherry wasn't insulting these excited woman.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3996</th>\n",
       "      <td>Sonia will dislike this lucky woman.</td>\n",
       "      <td>Sonia will dislike this lucky women.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3997</th>\n",
       "      <td>Those art galleries irritate this clever woman.</td>\n",
       "      <td>Those art galleries irritate this clever women.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3998</th>\n",
       "      <td>Amy won't work with this unconvinced man.</td>\n",
       "      <td>Amy won't work with this unconvinced men.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3999</th>\n",
       "      <td>Tara can't find that lost fungus.</td>\n",
       "      <td>Tara can't find that lost fungi.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        sentence_good  \\\n",
       "0                     Raymond is selling this sketch.   \n",
       "1                  Craig explored that grocery store.   \n",
       "2                      Eva has scared these children.   \n",
       "3                 Marcus would conceal that pamphlet.   \n",
       "4              Carmen hadn't shocked these customers.   \n",
       "...                                               ...   \n",
       "3995     Sherry wasn't insulting these excited women.   \n",
       "3996             Sonia will dislike this lucky woman.   \n",
       "3997  Those art galleries irritate this clever woman.   \n",
       "3998        Amy won't work with this unconvinced man.   \n",
       "3999                Tara can't find that lost fungus.   \n",
       "\n",
       "                                         sentence_bad  \n",
       "0                   Raymond is selling this sketches.  \n",
       "1                 Craig explored that grocery stores.  \n",
       "2                         Eva has scared these child.  \n",
       "3                Marcus would conceal that pamphlets.  \n",
       "4               Carmen hadn't shocked these customer.  \n",
       "...                                               ...  \n",
       "3995     Sherry wasn't insulting these excited woman.  \n",
       "3996             Sonia will dislike this lucky women.  \n",
       "3997  Those art galleries irritate this clever women.  \n",
       "3998        Amy won't work with this unconvinced men.  \n",
       "3999                 Tara can't find that lost fungi.  \n",
       "\n",
       "[4000 rows x 2 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = get_data(datasets[phenomenon_index])\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b94351e1-83f3-40ae-a8c0-82dc2e0d2ed8",
   "metadata": {
    "tags": [],
    "user_expressions": []
   },
   "source": [
    "### 2. Prepare argument structure data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "09668026-f097-478e-9bf3-399618d4b2a5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_main_verb(sentence):\n",
    "    # for argument structure\n",
    "    doc = nlp(sentence)\n",
    "    main_verbs = []\n",
    "\n",
    "    for token in doc:\n",
    "        if token.pos_ == \"VERB\":\n",
    "            main_verbs.append(token.text)\n",
    "\n",
    "    assert len(main_verbs) == 1\n",
    "\n",
    "    return main_verbs[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ca9ed4e-b783-4d3b-91ad-e9c280f6cd25",
   "metadata": {
    "tags": [],
    "user_expressions": []
   },
   "source": [
    "### 3. Prepare determiner-noun agreement data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "65733fab-7532-4654-8714-01b61a9b4c4c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def extract_determiner_from_target_noun(text, target_noun):\n",
    "    # for determiner-noun agreement\n",
    "\n",
    "    text += \" \" + target_noun\n",
    "    doc = nlp(text)\n",
    "\n",
    "    for token in doc:\n",
    "        if token.text.lower() == target_noun.lower() and token.pos_ == \"NOUN\":\n",
    "            for child in token.children:\n",
    "                if child.dep_ == \"det\":\n",
    "                    return child.text\n",
    "    return \"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35f109d6-d76f-427a-b94a-bdf9803191a6",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "### 4. Prepare NPI licensing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9f5a721e-c7d0-4b65-ae44-bb745ca0a046",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def extract_even(text):\n",
    "    # for npi licensing\n",
    "    # even is the first word of all the examples in our dataset\n",
    "\n",
    "    return text.split(\" \")[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20726d96-b208-4e9c-a28d-7e997758551c",
   "metadata": {
    "tags": [],
    "user_expressions": []
   },
   "source": [
    "### 5. Prepare subject-verb agreement data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "54422955-8b32-4cfe-afd5-cc0fe5dae123",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_verb_from_sentence(text):\n",
    "    # for subject-verb agreement\n",
    "\n",
    "    doc = nlp(text)\n",
    "    for token in doc:\n",
    "        if token.text.lower() and (token.pos_ == \"VERB\" or token.pos_ == \"AUX\"):\n",
    "            return token.text\n",
    "\n",
    "    return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "86ebb52f-7573-4623-8260-88c62cd65a76",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def extract_subject_from_target_verb(text, correct_word):\n",
    "    text += \" \" + correct_word\n",
    "    target_verb = get_verb_from_sentence(text)\n",
    "\n",
    "    if target_verb == \"\":\n",
    "        return target_verb\n",
    "    doc = nlp(text)\n",
    "    for token in doc:\n",
    "        if token.text.lower() == target_verb.lower() and (\n",
    "            token.pos_ == \"VERB\" or token.pos_ == \"AUX\"\n",
    "        ):\n",
    "            for child in token.children:\n",
    "                if child.dep_ in {\"nsubj\", \"nsubjpass\"}:\n",
    "                    return child.text\n",
    "    return \"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "685bf7b1-c33e-4c21-9233-c62bbc614d78",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def tokenize_sentences(good_sentence, bad_sentence):\n",
    "    # remove characters that are not letters or apostrophes and replace contractions\n",
    "    good_sentence_cleaned = re.sub(r\"[^\\w\\s']\", \"\", good_sentence).replace(\"n't\", \" not\")\n",
    "    bad_sentence_cleaned = re.sub(r\"[^\\w\\s']\", \"\", bad_sentence).replace(\"n't\", \" not\")\n",
    "\n",
    "    # tokenize the text\n",
    "    good_sentence_tokenized = good_sentence_cleaned.split(\" \")\n",
    "    bad_sentence_tokenized = bad_sentence_cleaned.split(\" \")\n",
    "\n",
    "    return good_sentence_tokenized, bad_sentence_tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4b8e37e6-9684-472d-8b31-f10697bb9fe7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_common_sentence(good_sentence_tokenized, bad_sentence_tokenized):\n",
    "    # consider only pairs of sentences with the same number of words\n",
    "    number_considered_words = min(len(good_sentence_tokenized), len(bad_sentence_tokenized))\n",
    "    different_lengths = len(good_sentence_tokenized) != len(bad_sentence_tokenized)\n",
    "\n",
    "    good_sentence_tokenized = good_sentence_tokenized[:number_considered_words]\n",
    "    bad_sentence_tokenized = bad_sentence_tokenized[:number_considered_words]\n",
    "\n",
    "    # get the common part of the two sentences (until the first different word)\n",
    "    same_tokens = np.array(good_sentence_tokenized) == np.array(bad_sentence_tokenized)\n",
    "    index_first_diff_token = np.where(same_tokens == False)[0][0]\n",
    "\n",
    "    common_sentence_tokenized = good_sentence_tokenized[:index_first_diff_token]\n",
    "    common_sentence = \" \".join(common_sentence_tokenized)\n",
    "\n",
    "    return (\n",
    "        common_sentence,\n",
    "        common_sentence_tokenized,\n",
    "        index_first_diff_token,\n",
    "        different_lengths,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "40214432-44cf-4967-8aa0-6936e283ce9b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_target(phenomenon_type, sentence, correct_word):\n",
    "    if phenomenon_type == \"argument structure\":\n",
    "        return get_main_verb(sentence)\n",
    "    elif phenomenon_type == \"determiner noun agreement\":\n",
    "        return extract_determiner_from_target_noun(sentence, correct_word)\n",
    "    elif phenomenon_type == \"npi licensing\":\n",
    "        return extract_even(sentence)\n",
    "    elif phenomenon_type == \"subject verb agreement\":\n",
    "        return extract_subject_from_target_verb(sentence, correct_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1cd0f7f0-c339-4d5f-abfd-96be969530e4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def preprocess_data(input_data, phenomenon_type):\n",
    "    good_bad_sentences = input_data.to_numpy()\n",
    "    extracted_sentences_data = []\n",
    "    dif_lengths_counter = 0\n",
    "\n",
    "    for good_sentence, bad_sentence in tqdm(good_bad_sentences):\n",
    "        good_sentence_tokenized, bad_sentence_tokenized = tokenize_sentences(good_sentence, bad_sentence)\n",
    "        sentence_info = get_common_sentence(good_sentence_tokenized, bad_sentence_tokenized)\n",
    "        common_sentence, common_sentence_tokenized, index_first_diff_token, different_lengths = sentence_info\n",
    "\n",
    "        if different_lengths:\n",
    "            dif_lengths_counter += 1\n",
    "\n",
    "        # get the correct and foil words\n",
    "        correct_word = good_sentence_tokenized[index_first_diff_token]\n",
    "        foil_word = bad_sentence_tokenized[index_first_diff_token]\n",
    "\n",
    "        # get the target token\n",
    "        target = get_target(phenomenon_type, common_sentence, correct_word)\n",
    "\n",
    "        if target == \"\":\n",
    "            continue\n",
    "\n",
    "        if len(good_sentence_tokenized) -1 == index_first_diff_token:\n",
    "            is_full_sentence = True\n",
    "        else:\n",
    "            is_full_sentence = False\n",
    "\n",
    "        # construct an array where 1s represent the position of the target word\n",
    "        known_evidence = np.zeros(len(common_sentence_tokenized))\n",
    "        evidence_index = np.where(np.array(common_sentence_tokenized) == target)[0][0]\n",
    "        known_evidence[evidence_index] = 1\n",
    "\n",
    "        extracted_data = [known_evidence, common_sentence, correct_word, foil_word, is_full_sentence]\n",
    "        extracted_sentences_data.append(extracted_data)\n",
    "\n",
    "    print(f\"The number of pairs with different number of words: {dif_lengths_counter}\")\n",
    "    print(f\"Number of processed senteces: {len(extracted_sentences_data)}/{len(good_bad_sentences)}\")\n",
    "\n",
    "    return extracted_sentences_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e506b9c7-1657-456c-8cba-9839a0ae0824",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4000/4000 [00:18<00:00, 212.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of pairs with different number of words: 0\n",
      "Number of processed senteces: 3823/4000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "extracted_sentences_data = preprocess_data(data, phenomena[phenomenon_index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "172bda0c-0298-48fc-83d4-fd09b24049f3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "gpt2model = GPT2Model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fa18c957-60e5-404b-8ed5-c693e1e58564",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def match_evidence_with_tokenization(space_tokenization, gpt_tokenization, known_evidence):\n",
    "    # this function adds 0s or 1s in the known evidence to match the list of GPT tokens\n",
    "\n",
    "    space_tokenization_index = 0\n",
    "    updated_known_evidence = []\n",
    "    accumulated_string = gpt_tokenization[0]\n",
    "\n",
    "    for gpt_tokenization_index, gpt_token in enumerate(gpt_tokenization[1:]):\n",
    "        if accumulated_string == space_tokenization[space_tokenization_index]:\n",
    "            updated_known_evidence.append(known_evidence[space_tokenization_index])\n",
    "            accumulated_string = gpt_tokenization[gpt_tokenization_index + 1]\n",
    "            space_tokenization_index += 1\n",
    "\n",
    "        else:\n",
    "            accumulated_string += gpt_token\n",
    "            updated_known_evidence.append(known_evidence[space_tokenization_index])\n",
    "\n",
    "    updated_known_evidence.append(known_evidence[space_tokenization_index])\n",
    "\n",
    "    return updated_known_evidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f28d6dfa-ea3f-4516-96ab-f6d3b76a77eb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "70868bfd-bfb3-40bf-bd6d-47b909cc418c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def compute_alignment_metrics(sentence_data, contrastive_method):\n",
    "    known_evidence = sentence_data[0]\n",
    "    sentence = sentence_data[1]\n",
    "    correct_word = sentence_data[2]\n",
    "    foil_word = sentence_data[3]\n",
    "\n",
    "    saliency_map = contrastive_method(sentence, correct_word, foil_word)\n",
    "    extracted_words = [explanation[0].strip() for explanation in saliency_map]\n",
    "    explanation = [explanation[1] for explanation in saliency_map]\n",
    "\n",
    "    if sentence.split(\" \") == extracted_words or math.isnan(saliency_map[0][1]):\n",
    "        return None\n",
    "\n",
    "    known_evidence = match_evidence_with_tokenization(sentence.split(\" \"), extracted_words, known_evidence)\n",
    "    mean_dot_product = compute_mean_dot_product([explanation], [known_evidence])\n",
    "    mean_probes_needed = compute_mean_probes_needed([explanation], [known_evidence])\n",
    "    mean_reciprocal_rank = compute_mean_reciprocal_rank([explanation], [known_evidence])\n",
    "\n",
    "    return saliency_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "05128905-b7c1-4776-a3b4-10c0c919e084",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_contrastive_method(index):\n",
    "    if index == 0:\n",
    "        return \"gradient norm\", gpt2model.get_contrastive_gradient_norm\n",
    "    elif index == 1:\n",
    "        return \"input x gradient\", gpt2model.get_contrastive_input_x_gradient\n",
    "    else:\n",
    "        return \"input erasure\", gpt2model.get_input_erasure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c114e5f2-8a84-458c-851a-e1f6023e88fc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def extract_form_data(extracted_sentences_data, phenomenon, number_of_examples=8):\n",
    "    index = 0\n",
    "    form_data = []\n",
    "    random.shuffle(extracted_sentences_data)\n",
    "\n",
    "    for sentence_data in extracted_sentences_data:\n",
    "        contrastive_function_name, contrastive_function = get_contrastive_method(index)\n",
    "        saliency_map = compute_alignment_metrics(sentence_data, contrastive_function)\n",
    "\n",
    "        if contrastive_function_name is None or saliency_map is None:\n",
    "            continue\n",
    "\n",
    "        form_data.append(\n",
    "            {\n",
    "                \"phenomenon\": phenomenon,\n",
    "                \"input sentence\": sentence_data[1],\n",
    "                \"correct word\": sentence_data[2],\n",
    "                \"wrong word\": sentence_data[3],\n",
    "                \"contrastive function\": contrastive_function_name,\n",
    "                \"explanation\": saliency_map,\n",
    "            }\n",
    "        )\n",
    "\n",
    "        index += 1\n",
    "\n",
    "        if index == number_of_examples:\n",
    "            break\n",
    "\n",
    "    return form_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "978ee114-635a-4262-9bbc-620401a86984",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "## Extract data for form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d6dff82b-aa52-4a16-9e0e-0b7d2b05b8de",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:04<00:00, 213.80it/s]\n",
      "/home/jovyan/XAI/project/gpt2_model.py:82: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  gradient_norm = torch.norm(torch.tensor(gradients[i]), p=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of pairs with different number of words: 12\n",
      "Number of processed senteces: 1000/1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4000/4000 [00:18<00:00, 215.62it/s]\n",
      "/home/jovyan/XAI/project/gpt2_model.py:82: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  gradient_norm = torch.norm(torch.tensor(gradients[i]), p=1)\n",
      "/home/jovyan/XAI/project/gpt2_model.py:139: RuntimeWarning: invalid value encountered in divide\n",
      "  normalized_probabilities = numpy_outputs/sum(numpy_outputs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of pairs with different number of words: 0\n",
      "Number of processed senteces: 3823/4000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:00<00:00, 54861.93it/s]\n",
      "/home/jovyan/XAI/project/gpt2_model.py:82: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  gradient_norm = torch.norm(torch.tensor(gradients[i]), p=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of pairs with different number of words: 0\n",
      "Number of processed senteces: 1000/1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3000/3000 [00:24<00:00, 124.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of pairs with different number of words: 0\n",
      "Number of processed senteces: 2572/3000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jovyan/XAI/project/gpt2_model.py:82: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  gradient_norm = torch.norm(torch.tensor(gradients[i]), p=1)\n",
      "/home/jovyan/XAI/project/gpt2_model.py:139: RuntimeWarning: invalid value encountered in divide\n",
      "  normalized_probabilities = numpy_outputs/sum(numpy_outputs)\n"
     ]
    }
   ],
   "source": [
    "global_form_data = []\n",
    "\n",
    "for phenomenon, datasets_for_phenomenon in zip(phenomena, datasets):\n",
    "    data = get_data(datasets_for_phenomenon)\n",
    "    extracted_sentences_data = preprocess_data(data, phenomenon)\n",
    "    global_form_data += extract_form_data(extracted_sentences_data, phenomenon)\n",
    "\n",
    "with open('./data/form_data.json', 'w') as f:\n",
    "    json.dump(global_form_data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "53de1bb0-7829-43cc-9098-4b57ff509d85",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_explanation(words, scores, name=\"explanation\"):\n",
    "    fig, ax = plt.subplots(1, 1)\n",
    "    fig.set_size_inches(1.2 * len(scores), 0.5)\n",
    "\n",
    "    rounded_scores = [round(score, 4) for score in scores]\n",
    "    xticks_positions = np.array(range(len(rounded_scores))) + 0.5\n",
    "\n",
    "    sns.heatmap(data=np.array([scores]), annot=labels, fmt=\"\", cmap='coolwarm', cbar=False, ax=ax)\n",
    "    ax.set_xticks(ticks=xticks_positions, labels=np.array(rounded_scores))\n",
    "    ax.set_yticks([])\n",
    "\n",
    "    plt.show()\n",
    "    fig.savefig(f\"./visual_explanations/{name}.png\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "11a7c092-e5e6-451e-ab86-8b6973ebd5e4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 840x50 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "words = [info[0] for info in global_form_data[10][\"explanation\"]]\n",
    "# words contains ['Am', 'elia', ' has', ' not', ' visited', ' this', ' exposed']\n",
    "scores = [info[1] for info in global_form_data[10][\"explanation\"]]\n",
    "# scores contains [-0.023513547215242535, 0.04307653600029133, 0.15159219643324162, 0.013719914235362795, 0.3185397058709933, 0.3534767672955558, 0.14310842737979765]\n",
    "\n",
    "plot_explanation(words, scores)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
