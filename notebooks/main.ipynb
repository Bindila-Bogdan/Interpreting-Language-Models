{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "363a078c-6808-4bf5-ad66-750c2047b5b4",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 0. Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7336fa36-a5dc-43d9-976e-427a9e8c4e9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1a29510a-b6f4-492a-b028-0776461b1199",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import sys\n",
    "import json\n",
    "import math\n",
    "import spacy\n",
    "import torch\n",
    "import random\n",
    "import warnings\n",
    "import numpy as np\n",
    "import transformers\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import seaborn as sns\n",
    "from functools import reduce\n",
    "from matplotlib import pyplot as plt\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# import local scripts\n",
    "sys.path.append('../src/')\n",
    "from data_loader import DatasetLoader\n",
    "from alignment_metrics import *\n",
    "from contrastive_explanations import ContrastiveExplanations\n",
    "contrastive_explanations = ContrastiveExplanations()\n",
    "\n",
    "# produce repeatable results\n",
    "np.random.seed(seed=42)\n",
    "transformers.set_seed(42)\n",
    "\n",
    "# enable CUDNN deterministic mode\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# suppress warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c4173ef-0513-4bb0-a170-2129d03d67e2",
   "metadata": {
    "tags": [],
    "user_expressions": []
   },
   "source": [
    "### 1. Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "89280074-a98f-410b-9bb6-db1882ec86da",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_data(datasets):\n",
    "    data_loader = DatasetLoader()\n",
    "\n",
    "    data = (\n",
    "        pd.concat(\n",
    "            [\n",
    "                pd.DataFrame(data_loader.load_data(dataset_name)[\"train\"])[\n",
    "                    [\"sentence_good\", \"sentence_bad\"]\n",
    "                ]\n",
    "                for dataset_name in datasets\n",
    "            ]\n",
    "        )\n",
    "        .reset_index()\n",
    "        .drop(\"index\", axis=1)\n",
    "    )\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bbb4353c-15dd-44cf-8143-0316945e87be",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence_good</th>\n",
       "      <th>sentence_bad</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Katherine can't help herself.</td>\n",
       "      <td>Katherine can't help himself.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Karla could listen to herself.</td>\n",
       "      <td>Karla could listen to himself.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Marie won't think about herself.</td>\n",
       "      <td>Marie won't think about itself.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Mark hasn't discussed himself.</td>\n",
       "      <td>Mark hasn't discussed itself.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Stephen impressed himself.</td>\n",
       "      <td>Stephen impressed itself.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>Carlos complained about himself.</td>\n",
       "      <td>Carlos complained about itself.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>Benjamin is firing himself.</td>\n",
       "      <td>Benjamin is firing itself.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>Chad kisses himself.</td>\n",
       "      <td>Chad kisses herself.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>Paula was arguing about herself.</td>\n",
       "      <td>Paula was arguing about himself.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>Colleen should question herself.</td>\n",
       "      <td>Colleen should question itself.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                        sentence_good                      sentence_bad\n",
       "0       Katherine can't help herself.     Katherine can't help himself.\n",
       "1      Karla could listen to herself.    Karla could listen to himself.\n",
       "2    Marie won't think about herself.   Marie won't think about itself.\n",
       "3      Mark hasn't discussed himself.     Mark hasn't discussed itself.\n",
       "4          Stephen impressed himself.         Stephen impressed itself.\n",
       "..                                ...                               ...\n",
       "995  Carlos complained about himself.   Carlos complained about itself.\n",
       "996       Benjamin is firing himself.        Benjamin is firing itself.\n",
       "997              Chad kisses himself.              Chad kisses herself.\n",
       "998  Paula was arguing about herself.  Paula was arguing about himself.\n",
       "999  Colleen should question herself.   Colleen should question itself.\n",
       "\n",
       "[1000 rows x 2 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# displays a phenomenon\n",
    "data = get_data([\"anaphor_gender_agreement\"])\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8407b0ca-1e93-43f8-9d52-8c33e9e6e500",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "### 1. Preprocess data for computing the contrastive explanations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d07baa3f-81b9-4e07-a970-df394ea8821b",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "#### 1.1. Define methods for extracting the target for each type of grammatical phenomenon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a5804fa7-26bc-4db7-b487-4010706a3ca3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def extract_reflexive_antecedent(text, target_reflexive):\n",
    "    # for anaphor agreement\n",
    "\n",
    "    text += \" \" + target_reflexive\n",
    "    doc = nlp(text)\n",
    "\n",
    "    target_token = None\n",
    "    for token in doc:\n",
    "        # reflexive pronouns usually have 'pobj' dependency\n",
    "        if token.text.lower() == target_reflexive.lower() and token.dep_ in [\"pobj\", \"dobj\"]:\n",
    "            target_token = token\n",
    "            break\n",
    "\n",
    "    if not target_token:\n",
    "        return \"\"\n",
    "\n",
    "    for token in target_token.head.lefts:\n",
    "        if token.dep_ in [\"nsubj\", \"nsubjpass\"] and token.pos_ in [\"NOUN\", \"PROPN\"]:\n",
    "            return token.text\n",
    "\n",
    "    return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "09668026-f097-478e-9bf3-399618d4b2a5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_main_verb(sentence):\n",
    "    # for argument structure\n",
    "\n",
    "    doc = nlp(sentence)\n",
    "    main_verbs = []\n",
    "\n",
    "    for token in doc:\n",
    "        if token.pos_ == \"VERB\":\n",
    "            main_verbs.append(token.text)\n",
    "\n",
    "    assert len(main_verbs) == 1\n",
    "\n",
    "    return main_verbs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "65733fab-7532-4654-8714-01b61a9b4c4c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def extract_determiner_from_target_noun(text, target_noun):\n",
    "    # for determiner-noun agreement\n",
    "\n",
    "    text += \" \" + target_noun\n",
    "    doc = nlp(text)\n",
    "\n",
    "    for token in doc:\n",
    "        if token.text.lower() == target_noun.lower() and token.pos_ in [\"NOUN\", \"PROPN\"]:\n",
    "            for child in token.children:\n",
    "                if child.dep_ == \"det\":\n",
    "                    return child.text\n",
    "\n",
    "    return \"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9f5a721e-c7d0-4b65-ae44-bb745ca0a046",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def extract_even(text):\n",
    "    # for npi licensing\n",
    "    # even is the first word of all the examples in our dataset\n",
    "\n",
    "    return text.split(\" \")[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "54422955-8b32-4cfe-afd5-cc0fe5dae123",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_verb_from_sentence(text):\n",
    "    doc = nlp(text)\n",
    "    for token in doc:\n",
    "        if token.text.lower() and (token.pos_ == \"VERB\" or token.pos_ == \"AUX\"):\n",
    "            return token.text\n",
    "\n",
    "    return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "86ebb52f-7573-4623-8260-88c62cd65a76",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def extract_subject_from_target_verb(text, correct_word):\n",
    "    # for subject-verb agreement\n",
    "\n",
    "    text += \" \" + correct_word\n",
    "    target_verb = get_verb_from_sentence(text)\n",
    "\n",
    "    if target_verb == \"\":\n",
    "        return target_verb\n",
    "    doc = nlp(text)\n",
    "    for token in doc:\n",
    "        if token.text.lower() == target_verb.lower() and (\n",
    "            token.pos_ == \"VERB\" or token.pos_ == \"AUX\"\n",
    "        ):\n",
    "            for child in token.children:\n",
    "                if child.dep_ in {\"nsubj\", \"nsubjpass\"}:\n",
    "                    return child.text\n",
    "    return \"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d8f821d-c9ad-4756-b021-9da0db7bb0a6",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "#### 1.2. Define auxiliary data processing operations and the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "685bf7b1-c33e-4c21-9233-c62bbc614d78",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def tokenize_sentences(good_sentence, bad_sentence):\n",
    "    # remove characters that are not letters or apostrophes and replace contractions\n",
    "    good_sentence_cleaned = re.sub(r\"[^\\w\\s']\", \"\", good_sentence).replace(\"n't\", \" not\")\n",
    "    bad_sentence_cleaned = re.sub(r\"[^\\w\\s']\", \"\", bad_sentence).replace(\"n't\", \" not\")\n",
    "\n",
    "    # tokenize the text\n",
    "    good_sentence_tokenized = good_sentence_cleaned.split(\" \")\n",
    "    bad_sentence_tokenized = bad_sentence_cleaned.split(\" \")\n",
    "\n",
    "    return good_sentence_tokenized, bad_sentence_tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4b8e37e6-9684-472d-8b31-f10697bb9fe7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_common_sentence(good_sentence_tokenized, bad_sentence_tokenized):\n",
    "    # consider only pairs of sentences with the same number of words\n",
    "    number_considered_words = min(len(good_sentence_tokenized), len(bad_sentence_tokenized))\n",
    "    different_lengths = len(good_sentence_tokenized) != len(bad_sentence_tokenized)\n",
    "\n",
    "    good_sentence_tokenized = good_sentence_tokenized[:number_considered_words]\n",
    "    bad_sentence_tokenized = bad_sentence_tokenized[:number_considered_words]\n",
    "\n",
    "    # get the common part of the two sentences (until the first different word)\n",
    "    same_tokens = np.array(good_sentence_tokenized) == np.array(bad_sentence_tokenized)\n",
    "    index_first_diff_token = np.where(same_tokens == False)[0][0]\n",
    "\n",
    "    common_sentence_tokenized = good_sentence_tokenized[:index_first_diff_token]\n",
    "    common_sentence = \" \".join(common_sentence_tokenized)\n",
    "\n",
    "    return (\n",
    "        common_sentence,\n",
    "        common_sentence_tokenized,\n",
    "        index_first_diff_token,\n",
    "        different_lengths,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "40214432-44cf-4967-8aa0-6936e283ce9b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_target(phenomenon_type, sentence, correct_word):\n",
    "    if phenomenon_type == \"anaphor agreement\":\n",
    "        return extract_reflexive_antecedent(sentence, correct_word)\n",
    "    elif phenomenon_type == \"argument structure\":\n",
    "        return get_main_verb(sentence)\n",
    "    elif phenomenon_type == \"determiner noun agreement\":\n",
    "        return extract_determiner_from_target_noun(sentence, correct_word)\n",
    "    elif phenomenon_type == \"npi licensing\":\n",
    "        return extract_even(sentence)\n",
    "    elif phenomenon_type == \"subject verb agreement\":\n",
    "        return extract_subject_from_target_verb(sentence, correct_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1cd0f7f0-c339-4d5f-abfd-96be969530e4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def preprocess_data(input_data, phenomenon_type):\n",
    "    good_bad_sentences = input_data.to_numpy()\n",
    "    extracted_sentences_data = []\n",
    "    dif_lengths_counter = 0\n",
    "\n",
    "    for good_sentence, bad_sentence in good_bad_sentences:\n",
    "        good_sentence_tokenized, bad_sentence_tokenized = tokenize_sentences(good_sentence, bad_sentence)\n",
    "        sentence_info = get_common_sentence(good_sentence_tokenized, bad_sentence_tokenized)\n",
    "        common_sentence, common_sentence_tokenized, index_first_diff_token, different_lengths = sentence_info\n",
    "\n",
    "        if different_lengths:\n",
    "            dif_lengths_counter += 1\n",
    "\n",
    "        # get the correct and foil words\n",
    "        correct_word = good_sentence_tokenized[index_first_diff_token]\n",
    "        foil_word = bad_sentence_tokenized[index_first_diff_token]\n",
    "\n",
    "        # get the target token\n",
    "        target = get_target(phenomenon_type, common_sentence, correct_word)\n",
    "\n",
    "        if target == \"\":\n",
    "            continue\n",
    "\n",
    "        if len(good_sentence_tokenized) -1 == index_first_diff_token:\n",
    "            is_full_sentence = True\n",
    "        else:\n",
    "            is_full_sentence = False\n",
    "\n",
    "        # construct an array where 1s represent the position of the target word\n",
    "        known_evidence = np.zeros(len(common_sentence_tokenized))\n",
    "        evidence_index = np.where(np.array(common_sentence_tokenized) == target)[0][0]\n",
    "        known_evidence[evidence_index] = 1\n",
    "\n",
    "        extracted_data = [known_evidence, common_sentence, correct_word, foil_word, is_full_sentence]\n",
    "        extracted_sentences_data.append(extracted_data)\n",
    "\n",
    "    return extracted_sentences_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4c235635-aa26-4100-8d7d-3457236b5eec",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def preprocess_all_data(phenomenon_names, dataset_names):\n",
    "    all_extracted_sentences_data = []\n",
    "    number_of_invalid_examples = []\n",
    "\n",
    "    for phenomenon_name, dataset_name in zip(phenomenon_names, dataset_names):\n",
    "        data = get_data([dataset_name])\n",
    "        extracted_sentences_data = preprocess_data(data, phenomenon_name)\n",
    "        all_extracted_sentences_data.extend(extracted_sentences_data)\n",
    "        number_of_invalid_examples.append(data.shape[0] - len(extracted_sentences_data))\n",
    "\n",
    "    return all_extracted_sentences_data, number_of_invalid_examples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64ba77bd-99f0-4aea-91e6-87633b8b01d0",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "### 2. Compute saliency maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fa18c957-60e5-404b-8ed5-c693e1e58564",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def match_evidence_with_tokenization(space_tokenization, gpt_tokenization, known_evidence):\n",
    "    # this function adds 0s or 1s in the known evidence to match the list of GPT tokens\n",
    "\n",
    "    space_tokenization_index = 0\n",
    "    updated_known_evidence = []\n",
    "    accumulated_string = gpt_tokenization[0]\n",
    "\n",
    "    for gpt_tokenization_index, gpt_token in enumerate(gpt_tokenization[1:]):\n",
    "        if accumulated_string == space_tokenization[space_tokenization_index]:\n",
    "            updated_known_evidence.append(known_evidence[space_tokenization_index])\n",
    "            accumulated_string = gpt_tokenization[gpt_tokenization_index + 1]\n",
    "            space_tokenization_index += 1\n",
    "\n",
    "        else:\n",
    "            accumulated_string += gpt_token\n",
    "            updated_known_evidence.append(known_evidence[space_tokenization_index])\n",
    "\n",
    "    updated_known_evidence.append(known_evidence[space_tokenization_index])\n",
    "\n",
    "    return updated_known_evidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "70868bfd-bfb3-40bf-bd6d-47b909cc418c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def compute_alignment_metrics(sentence_data, contrastive_method):\n",
    "    known_evidence = sentence_data[0]\n",
    "    sentence = sentence_data[1]\n",
    "    correct_word = sentence_data[2]\n",
    "    foil_word = sentence_data[3]\n",
    "\n",
    "    saliency_map = contrastive_method(sentence, correct_word, foil_word)\n",
    "    extracted_words = [explanation[0].strip() for explanation in saliency_map]\n",
    "    explanation = [explanation[1] for explanation in saliency_map]\n",
    "\n",
    "    if math.isnan(saliency_map[0][1]):\n",
    "        return None, None, None, None\n",
    "\n",
    "    known_evidence = match_evidence_with_tokenization(sentence.split(\" \"), extracted_words, known_evidence)\n",
    "    mean_dot_product = compute_mean_dot_product([explanation], [known_evidence])\n",
    "    mean_probes_needed = compute_mean_probes_needed([explanation], [known_evidence])\n",
    "    mean_reciprocal_rank = compute_mean_reciprocal_rank([explanation], [known_evidence])\n",
    "\n",
    "    return saliency_map, mean_dot_product, mean_probes_needed, mean_reciprocal_rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "05128905-b7c1-4776-a3b4-10c0c919e084",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_contrastive_method(index, contrastive_explanations):\n",
    "    if index == 0:\n",
    "        return \"gradient norm\", contrastive_explanations.get_contrastive_gradient_norm\n",
    "    elif index == 1:\n",
    "        return \"input x gradient\", contrastive_explanations.get_contrastive_input_x_gradient\n",
    "    else:\n",
    "        return \"input erasure\", contrastive_explanations.get_input_erasure"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "978ee114-635a-4262-9bbc-620401a86984",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "### 3. Extract data for form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "767d7dfe-c033-4907-9963-ba445c50ec8f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "anaphor_agreement_datasets = [\n",
    "    \"anaphor_gender_agreement\",\n",
    "    \"anaphor_number_agreement\",\n",
    "]\n",
    "\n",
    "argument_structure_datasets = [\"animate_subject_passive\"]\n",
    "\n",
    "determiner_noun_agreement_datasets = [\n",
    "    \"determiner_noun_agreement_1\",\n",
    "    \"determiner_noun_agreement_irregular_1\",\n",
    "    \"determiner_noun_agreement_with_adjective_1\",\n",
    "    \"determiner_noun_agreement_with_adj_irregular_1\",\n",
    "]\n",
    "\n",
    "npi_licesing_datasets = [\n",
    "    \"npi_present_1\",\n",
    "]\n",
    "\n",
    "subject_verb_agreement_datasets = [\n",
    "    \"distractor_agreement_relational_noun\",\n",
    "    \"irregular_plural_subject_verb_agreement_1\",\n",
    "    \"regular_plural_subject_verb_agreement_1\",\n",
    "]\n",
    "\n",
    "phenomena = [\n",
    "    \"anaphor agreement\",\n",
    "    \"argument structure\",\n",
    "    \"determiner noun agreement\",\n",
    "    \"npi licensing\",\n",
    "    \"subject verb agreement\",\n",
    "]\n",
    "\n",
    "datasets = [\n",
    "    anaphor_agreement_datasets,\n",
    "    argument_structure_datasets,\n",
    "    determiner_noun_agreement_datasets,\n",
    "    npi_licesing_datasets,\n",
    "    subject_verb_agreement_datasets,\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c114e5f2-8a84-458c-851a-e1f6023e88fc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def extract_form_data(extracted_sentences_data, phenomenon, contrastive_explanations, number_of_examples=8):\n",
    "    index = 0\n",
    "    form_data = []\n",
    "    random.shuffle(extracted_sentences_data)\n",
    "\n",
    "    for sentence_data in extracted_sentences_data:\n",
    "        contrastive_function_name, contrastive_function = get_contrastive_method(index, contrastive_explanations)\n",
    "        saliency_map, _, _, _ = compute_alignment_metrics(sentence_data, contrastive_function)\n",
    "\n",
    "        if contrastive_function_name is None or saliency_map is None:\n",
    "            continue\n",
    "\n",
    "        form_data.append(\n",
    "            {\n",
    "                \"phenomenon\": phenomenon,\n",
    "                \"input sentence\": sentence_data[1],\n",
    "                \"correct word\": sentence_data[2],\n",
    "                \"wrong word\": sentence_data[3],\n",
    "                \"contrastive function\": contrastive_function_name,\n",
    "                \"explanation\": saliency_map,\n",
    "            }\n",
    "        )\n",
    "\n",
    "        index += 1\n",
    "\n",
    "        if index == number_of_examples:\n",
    "            break\n",
    "\n",
    "    return form_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d6dff82b-aa52-4a16-9e0e-0b7d2b05b8de",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "global_form_data = []\n",
    "\n",
    "for phenomenon, datasets_for_phenomenon in zip(phenomena, datasets):\n",
    "    data = get_data(datasets_for_phenomenon)\n",
    "    extracted_sentences_data = preprocess_data(data, phenomenon)\n",
    "    global_form_data += extract_form_data(extracted_sentences_data, phenomenon, contrastive_explanations)\n",
    "\n",
    "with open('./data/form_data.json', 'w') as f:\n",
    "    json.dump(global_form_data, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "102e003d-b1b9-4316-a8fe-c7c4d34ca4a7",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "### 4. Compute alignment metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e596cb08-46ac-4575-b2d7-c403d03e1651",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset_names = [\n",
    "    \"anaphor_gender_agreement\",\n",
    "    \"anaphor_number_agreement\",\n",
    "    \"animate_subject_passive\",\n",
    "    \"determiner_noun_agreement_1\",\n",
    "    \"determiner_noun_agreement_irregular_1\",\n",
    "    \"determiner_noun_agreement_with_adjective_1\",\n",
    "    \"determiner_noun_agreement_with_adj_irregular_1\",\n",
    "    \"npi_present_1\",\n",
    "    \"distractor_agreement_relational_noun\",\n",
    "    \"irregular_plural_subject_verb_agreement_1\",\n",
    "    \"regular_plural_subject_verb_agreement_1\",\n",
    "]\n",
    "\n",
    "phenomenon_names = [\n",
    "    \"anaphor agreement\",\n",
    "    \"anaphor agreement\",\n",
    "    \"argument structure\",\n",
    "    \"determiner noun agreement\",\n",
    "    \"determiner noun agreement\",\n",
    "    \"determiner noun agreement\",\n",
    "    \"determiner noun agreement\",\n",
    "    \"npi licensing\",\n",
    "    \"subject verb agreement\",\n",
    "    \"subject verb agreement\",\n",
    "    \"subject verb agreement\",\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0a79105f-85aa-45f3-b0d7-3bbc336cba28",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_metric_entire_dataset(all_extracted_sentences_data, contrastive_method, contrastive_method_name):\n",
    "    explanations = []\n",
    "    known_evidences = []\n",
    "\n",
    "    for sentence_data in tqdm(all_extracted_sentences_data):\n",
    "        known_evidence = sentence_data[0]\n",
    "        sentence = sentence_data[1]\n",
    "        correct_word = sentence_data[2]\n",
    "        foil_word = sentence_data[3]\n",
    "\n",
    "        saliency_map = contrastive_method(sentence, correct_word, foil_word)\n",
    "        extracted_words = [explanation[0].strip() for explanation in saliency_map]\n",
    "        explanation = [explanation[1] for explanation in saliency_map]\n",
    "\n",
    "        if math.isnan(saliency_map[0][1]):\n",
    "            continue\n",
    "\n",
    "        known_evidence = match_evidence_with_tokenization(sentence.split(\" \"), extracted_words, known_evidence)\n",
    "\n",
    "        explanations.append(explanation)\n",
    "        known_evidences.append(known_evidence)\n",
    "\n",
    "    print(f\"Number of sentences considered in the end by {contrastive_method_name}: {len(explanations)}\")\n",
    "    mean_dot_product = compute_mean_dot_product(explanations, known_evidences)\n",
    "    mean_probes_needed = compute_mean_probes_needed(explanations, known_evidences)\n",
    "    mean_reciprocal_rank = compute_mean_reciprocal_rank(explanations, known_evidences)\n",
    "\n",
    "    return mean_dot_product, mean_probes_needed, mean_reciprocal_rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "772890c4-a7ed-41a7-95e9-691f7be37046",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_metrics_entire_dataset(all_extracted_sentences_datam, contrastive_explanations, results_name):\n",
    "    overall_metrics = {}\n",
    "\n",
    "    for contrastive_method_index in range(3):\n",
    "        contrastive_method_name, contrastive_method = get_contrastive_method(contrastive_method_index, contrastive_explanations)\n",
    "        mean_dot_product, mean_probes_needed, mean_reciprocal_rank = get_metric_entire_dataset(all_extracted_sentences_data, contrastive_method, contrastive_method_name)\n",
    "\n",
    "        overall_metrics[f\"mean dot product - {contrastive_method_name}\"] = mean_dot_product\n",
    "        overall_metrics[f\"mean probes needed - {contrastive_method_name}\"] = mean_probes_needed\n",
    "        overall_metrics[f\"mean reciprocal rank - {contrastive_method_name}\"] = mean_reciprocal_rank\n",
    "\n",
    "        with open(f'../results/{results_name}.json', 'w') as f:\n",
    "            json.dump(overall_metrics, f)\n",
    "\n",
    "    return overall_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cfee0e5-22ac-4b60-b466-2b58f76735f2",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "#### 4.1. Averaged across the entire dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "65b4d036-7bac-49fb-89e5-57454d3ea1d9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of invalid examples per phenomenon: [208, 205, 0, 17, 55, 33, 67, 0, 135, 146, 147]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9987/9987 [30:17<00:00,  5.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sentences considered in the end by gradient norm: 7793\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9987/9987 [30:28<00:00,  5.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sentences considered in the end by input x gradient: 7793\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9987/9987 [54:04<00:00,  3.08it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sentences considered in the end by input erasure: 7793\n"
     ]
    }
   ],
   "source": [
    "all_extracted_sentences_data, number_of_invalid_examples = preprocess_all_data(phenomenon_names, dataset_names)\n",
    "print(f\"Total number of invalid examples per phenomenon: {number_of_invalid_examples}\")\n",
    "results_name = \"averaged_metrics_all_data\"\n",
    "overall_metrics = get_metrics_entire_dataset(all_extracted_sentences_data, contrastive_explanations, results_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0769d3d7-7a55-40af-9f7a-55d4ef59e367",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "#### 4.2. Averaged per phenomenon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "59e58266-5582-4035-81e3-dddede710113",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "anaphor agreement\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1587/1587 [04:28<00:00,  5.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sentences considered in the end by gradient norm: 1587\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1587/1587 [04:36<00:00,  5.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sentences considered in the end by input x gradient: 1587\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1587/1587 [07:28<00:00,  3.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sentences considered in the end by input erasure: 1587\n",
      "argument structure\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [02:58<00:00,  5.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sentences considered in the end by gradient norm: 1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [03:07<00:00,  5.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sentences considered in the end by input x gradient: 1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [08:27<00:00,  1.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sentences considered in the end by input erasure: 1000\n",
      "determiner noun agreement\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3828/3828 [11:31<00:00,  5.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sentences considered in the end by gradient norm: 2037\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3828/3828 [11:32<00:00,  5.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sentences considered in the end by input x gradient: 2037\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3828/3828 [25:22<00:00,  2.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sentences considered in the end by input erasure: 2037\n",
      "npi licensing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [02:56<00:00,  5.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sentences considered in the end by gradient norm: 1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [02:53<00:00,  5.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sentences considered in the end by input x gradient: 1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [03:35<00:00,  4.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sentences considered in the end by input erasure: 1000\n",
      "subject verb agreement\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2572/2572 [07:58<00:00,  5.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sentences considered in the end by gradient norm: 2169\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2572/2572 [07:56<00:00,  5.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sentences considered in the end by input x gradient: 2169\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2572/2572 [09:47<00:00,  4.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sentences considered in the end by input erasure: 2169\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for phenomenon_index in range(len(phenomena)):\n",
    "    phenomenon_name = phenomena[phenomenon_index]\n",
    "    print(phenomenon_name)\n",
    "\n",
    "    datasets_no_per_phenomenon = len(datasets[phenomenon_index])\n",
    "    phenomenon_list = [phenomenon_name] * datasets_no_per_phenomenon \n",
    "    all_extracted_sentences_data, number_of_invalid_examples = preprocess_all_data(phenomenon_list, datasets[phenomenon_index])\n",
    "\n",
    "    results_name = \"averaged_metrics_\" + phenomenon_name.replace(\" \", \"_\")\n",
    "    _ = get_metrics_entire_dataset(all_extracted_sentences_data, contrastive_explanations, results_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41340878-51a5-4bbe-adf5-4666df483eac",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "### 5. Plot explanation (saliency map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "542c148b-3e31-4659-9680-6e4a28bebaa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_explanation(words, scores, name=\"explanation_example\"):\n",
    "    fig, ax = plt.subplots(1, 1)\n",
    "    fig.set_size_inches(1.2 * len(scores), 0.5)\n",
    "\n",
    "    rounded_scores = [round(score, 4) for score in scores]\n",
    "    xticks_positions = np.array(range(len(rounded_scores))) + 0.5\n",
    "\n",
    "    sns.heatmap(data=np.array([scores]), annot=np.array([words]), fmt=\"\", cmap='coolwarm', cbar=False, ax=ax)\n",
    "    ax.set_xticks(ticks=xticks_positions, labels=np.array(rounded_scores))\n",
    "    ax.set_yticks([])\n",
    "\n",
    "    plt.show()\n",
    "    fig.savefig(f\"../results/{name}.png\", bbox_inches='tight') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "88209e20-8ae5-4bdb-945a-e4b7dc5c4d3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x50 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# this is a explanation example\n",
    "words = [info[0] for info in global_form_data[9][\"explanation\"]]\n",
    "scores = [info[1] for info in global_form_data[9][\"explanation\"]]\n",
    "plot_explanation(words, scores)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
