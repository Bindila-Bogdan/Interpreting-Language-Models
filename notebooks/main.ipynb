{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "363a078c-6808-4bf5-ad66-750c2047b5b4",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 0. Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7336fa36-a5dc-43d9-976e-427a9e8c4e9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1a29510a-b6f4-492a-b028-0776461b1199",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import sys\n",
    "import json\n",
    "import math\n",
    "import spacy\n",
    "import torch\n",
    "import random\n",
    "import warnings\n",
    "import numpy as np\n",
    "import transformers\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import seaborn as sns\n",
    "from functools import reduce\n",
    "from matplotlib import pyplot as plt\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# import local scripts\n",
    "sys.path.append('../src/')\n",
    "from data_loader import DatasetLoader\n",
    "from alignment_metrics import *\n",
    "from contrastive_explanations import ContrastiveExplanations\n",
    "contrastive_explanations = ContrastiveExplanations()\n",
    "\n",
    "# produce repeatable results\n",
    "np.random.seed(seed=42)\n",
    "transformers.set_seed(42)\n",
    "\n",
    "# enable CUDNN deterministic mode\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# suppress warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c4173ef-0513-4bb0-a170-2129d03d67e2",
   "metadata": {
    "tags": [],
    "user_expressions": []
   },
   "source": [
    "### 1. Load the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3b8d313-2265-4d12-8d46-b8f961cbc7e7",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "Below are the names of the datasets and phenomena used by the authors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8bb67dff-7496-415c-b1b2-978768144041",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "anaphor_agreement_datasets = [\n",
    "    \"anaphor_gender_agreement\",\n",
    "    \"anaphor_number_agreement\",\n",
    "]\n",
    "\n",
    "argument_structure_datasets = [\"animate_subject_passive\"]\n",
    "\n",
    "determiner_noun_agreement_datasets = [\n",
    "    \"determiner_noun_agreement_1\",\n",
    "    \"determiner_noun_agreement_irregular_1\",\n",
    "    \"determiner_noun_agreement_with_adjective_1\",\n",
    "    \"determiner_noun_agreement_with_adj_irregular_1\",\n",
    "]\n",
    "\n",
    "npi_licesing_datasets = [\n",
    "    \"npi_present_1\",\n",
    "]\n",
    "\n",
    "subject_verb_agreement_datasets = [\n",
    "    \"distractor_agreement_relational_noun\",\n",
    "    \"irregular_plural_subject_verb_agreement_1\",\n",
    "    \"regular_plural_subject_verb_agreement_1\",\n",
    "]\n",
    "\n",
    "phenomena = [\n",
    "    \"anaphor agreement\",\n",
    "    \"argument structure\",\n",
    "    \"determiner noun agreement\",\n",
    "    \"npi licensing\",\n",
    "    \"subject verb agreement\",\n",
    "]\n",
    "datasets = [\n",
    "    anaphor_agreement_datasets,\n",
    "    argument_structure_datasets,\n",
    "    determiner_noun_agreement_datasets,\n",
    "    npi_licesing_datasets,\n",
    "    subject_verb_agreement_datasets,\n",
    "]\n",
    "phenomenon_index = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "89280074-a98f-410b-9bb6-db1882ec86da",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_data(datasets):\n",
    "    data_loader = DatasetLoader()\n",
    "\n",
    "    data = (\n",
    "        pd.concat(\n",
    "            [\n",
    "                pd.DataFrame(data_loader.load_data(dataset_name)[\"train\"])[\n",
    "                    [\"sentence_good\", \"sentence_bad\"]\n",
    "                ]\n",
    "                for dataset_name in datasets\n",
    "            ]\n",
    "        )\n",
    "        .reset_index()\n",
    "        .drop(\"index\", axis=1)\n",
    "    )\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "bbb4353c-15dd-44cf-8143-0316945e87be",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence_good</th>\n",
       "      <th>sentence_bad</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Katherine can't help herself.</td>\n",
       "      <td>Katherine can't help himself.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Karla could listen to herself.</td>\n",
       "      <td>Karla could listen to himself.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Marie won't think about herself.</td>\n",
       "      <td>Marie won't think about itself.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Mark hasn't discussed himself.</td>\n",
       "      <td>Mark hasn't discussed itself.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Stephen impressed himself.</td>\n",
       "      <td>Stephen impressed itself.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1995</th>\n",
       "      <td>Those guys cured themselves.</td>\n",
       "      <td>Those guys cured itself.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1996</th>\n",
       "      <td>Tara wasn't referencing herself.</td>\n",
       "      <td>Tara wasn't referencing themselves.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997</th>\n",
       "      <td>Most students attacked themselves.</td>\n",
       "      <td>Most students attacked itself.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1998</th>\n",
       "      <td>A lot of actors upset themselves.</td>\n",
       "      <td>A lot of actors upset herself.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1999</th>\n",
       "      <td>Keith could respect himself.</td>\n",
       "      <td>Keith could respect themselves.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                           sentence_good                         sentence_bad\n",
       "0          Katherine can't help herself.        Katherine can't help himself.\n",
       "1         Karla could listen to herself.       Karla could listen to himself.\n",
       "2       Marie won't think about herself.      Marie won't think about itself.\n",
       "3         Mark hasn't discussed himself.        Mark hasn't discussed itself.\n",
       "4             Stephen impressed himself.            Stephen impressed itself.\n",
       "...                                  ...                                  ...\n",
       "1995        Those guys cured themselves.             Those guys cured itself.\n",
       "1996    Tara wasn't referencing herself.  Tara wasn't referencing themselves.\n",
       "1997  Most students attacked themselves.       Most students attacked itself.\n",
       "1998   A lot of actors upset themselves.       A lot of actors upset herself.\n",
       "1999        Keith could respect himself.      Keith could respect themselves.\n",
       "\n",
       "[2000 rows x 2 columns]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = get_data(datasets[phenomenon_index])\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8407b0ca-1e93-43f8-9d52-8c33e9e6e500",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "### 1. Preprocess data for computing the contrastive explanations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d07baa3f-81b9-4e07-a970-df394ea8821b",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "#### 1.1. Define methods for extracting the target for each type of grammatical phenomenon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "a5804fa7-26bc-4db7-b487-4010706a3ca3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def extract_reflexive_antecedent(text, target_reflexive):\n",
    "    # for anaphor agreement\n",
    "\n",
    "    text += \" \" + target_reflexive\n",
    "    doc = nlp(text)\n",
    "\n",
    "    target_token = None\n",
    "    for token in doc:\n",
    "        # reflexive pronouns usually have 'pobj' dependency\n",
    "        if token.text.lower() == target_reflexive.lower() and token.dep_ in [\"pobj\", \"dobj\"]:\n",
    "            target_token = token\n",
    "            break\n",
    "\n",
    "    if not target_token:\n",
    "        return \"\"\n",
    "\n",
    "    for token in target_token.head.lefts:\n",
    "        if token.dep_ in [\"nsubj\", \"nsubjpass\"] and token.pos_ in [\"NOUN\", \"PROPN\"]:\n",
    "            return token.text\n",
    "\n",
    "    return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "09668026-f097-478e-9bf3-399618d4b2a5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_main_verb(sentence):\n",
    "    # for argument structure\n",
    "\n",
    "    doc = nlp(sentence)\n",
    "    main_verbs = []\n",
    "\n",
    "    for token in doc:\n",
    "        if token.pos_ == \"VERB\":\n",
    "            main_verbs.append(token.text)\n",
    "\n",
    "    assert len(main_verbs) == 1\n",
    "\n",
    "    return main_verbs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "65733fab-7532-4654-8714-01b61a9b4c4c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def extract_determiner_from_target_noun(text, target_noun):\n",
    "    # for determiner-noun agreement\n",
    "\n",
    "    text += \" \" + target_noun\n",
    "    doc = nlp(text)\n",
    "\n",
    "    for token in doc:\n",
    "        if token.text.lower() == target_noun.lower() and token.pos_ in [\"NOUN\", \"PROPN\"]:\n",
    "            for child in token.children:\n",
    "                if child.dep_ == \"det\":\n",
    "                    return child.text\n",
    "\n",
    "    return \"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "9f5a721e-c7d0-4b65-ae44-bb745ca0a046",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def extract_even(text):\n",
    "    # for npi licensing\n",
    "    # even is the first word of all the examples in our dataset\n",
    "\n",
    "    return text.split(\" \")[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "54422955-8b32-4cfe-afd5-cc0fe5dae123",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_verb_from_sentence(text):\n",
    "    doc = nlp(text)\n",
    "    for token in doc:\n",
    "        if token.text.lower() and (token.pos_ == \"VERB\" or token.pos_ == \"AUX\"):\n",
    "            return token.text\n",
    "\n",
    "    return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "86ebb52f-7573-4623-8260-88c62cd65a76",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def extract_subject_from_target_verb(text, correct_word):\n",
    "    # for subject-verb agreement\n",
    "\n",
    "    text += \" \" + correct_word\n",
    "    target_verb = get_verb_from_sentence(text)\n",
    "\n",
    "    if target_verb == \"\":\n",
    "        return target_verb\n",
    "    doc = nlp(text)\n",
    "    for token in doc:\n",
    "        if token.text.lower() == target_verb.lower() and (\n",
    "            token.pos_ == \"VERB\" or token.pos_ == \"AUX\"\n",
    "        ):\n",
    "            for child in token.children:\n",
    "                if child.dep_ in {\"nsubj\", \"nsubjpass\"}:\n",
    "                    return child.text\n",
    "    return \"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d8f821d-c9ad-4756-b021-9da0db7bb0a6",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "#### 1.2. Define auxiliary data processing operations and the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "685bf7b1-c33e-4c21-9233-c62bbc614d78",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def tokenize_sentences(good_sentence, bad_sentence):\n",
    "    # remove characters that are not letters or apostrophes and replace contractions\n",
    "    good_sentence_cleaned = re.sub(r\"[^\\w\\s']\", \"\", good_sentence).replace(\"n't\", \" not\")\n",
    "    bad_sentence_cleaned = re.sub(r\"[^\\w\\s']\", \"\", bad_sentence).replace(\"n't\", \" not\")\n",
    "\n",
    "    # tokenize the text\n",
    "    good_sentence_tokenized = good_sentence_cleaned.split(\" \")\n",
    "    bad_sentence_tokenized = bad_sentence_cleaned.split(\" \")\n",
    "\n",
    "    return good_sentence_tokenized, bad_sentence_tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "4b8e37e6-9684-472d-8b31-f10697bb9fe7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_common_sentence(good_sentence_tokenized, bad_sentence_tokenized):\n",
    "    # consider only pairs of sentences with the same number of words\n",
    "    number_considered_words = min(len(good_sentence_tokenized), len(bad_sentence_tokenized))\n",
    "    different_lengths = len(good_sentence_tokenized) != len(bad_sentence_tokenized)\n",
    "\n",
    "    good_sentence_tokenized = good_sentence_tokenized[:number_considered_words]\n",
    "    bad_sentence_tokenized = bad_sentence_tokenized[:number_considered_words]\n",
    "\n",
    "    # get the common part of the two sentences (until the first different word)\n",
    "    same_tokens = np.array(good_sentence_tokenized) == np.array(bad_sentence_tokenized)\n",
    "    index_first_diff_token = np.where(same_tokens == False)[0][0]\n",
    "\n",
    "    common_sentence_tokenized = good_sentence_tokenized[:index_first_diff_token]\n",
    "    common_sentence = \" \".join(common_sentence_tokenized)\n",
    "\n",
    "    return (\n",
    "        common_sentence,\n",
    "        common_sentence_tokenized,\n",
    "        index_first_diff_token,\n",
    "        different_lengths,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "40214432-44cf-4967-8aa0-6936e283ce9b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_target(phenomenon_type, sentence, correct_word):\n",
    "    if phenomenon_type == \"anaphor agreement\":\n",
    "        return extract_reflexive_antecedent(sentence, correct_word)\n",
    "    elif phenomenon_type == \"argument structure\":\n",
    "        return get_main_verb(sentence)\n",
    "    elif phenomenon_type == \"determiner noun agreement\":\n",
    "        return extract_determiner_from_target_noun(sentence, correct_word)\n",
    "    elif phenomenon_type == \"npi licensing\":\n",
    "        return extract_even(sentence)\n",
    "    elif phenomenon_type == \"subject verb agreement\":\n",
    "        return extract_subject_from_target_verb(sentence, correct_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "1cd0f7f0-c339-4d5f-abfd-96be969530e4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def preprocess_data(input_data, phenomenon_type):\n",
    "    good_bad_sentences = input_data.to_numpy()\n",
    "    extracted_sentences_data = []\n",
    "    dif_lengths_counter = 0\n",
    "\n",
    "    for good_sentence, bad_sentence in tqdm(good_bad_sentences):\n",
    "        good_sentence_tokenized, bad_sentence_tokenized = tokenize_sentences(good_sentence, bad_sentence)\n",
    "        sentence_info = get_common_sentence(good_sentence_tokenized, bad_sentence_tokenized)\n",
    "        common_sentence, common_sentence_tokenized, index_first_diff_token, different_lengths = sentence_info\n",
    "\n",
    "        if different_lengths:\n",
    "            dif_lengths_counter += 1\n",
    "\n",
    "        # get the correct and foil words\n",
    "        correct_word = good_sentence_tokenized[index_first_diff_token]\n",
    "        foil_word = bad_sentence_tokenized[index_first_diff_token]\n",
    "\n",
    "        # get the target token\n",
    "        target = get_target(phenomenon_type, common_sentence, correct_word)\n",
    "\n",
    "        if target == \"\":\n",
    "            continue\n",
    "\n",
    "        if len(good_sentence_tokenized) -1 == index_first_diff_token:\n",
    "            is_full_sentence = True\n",
    "        else:\n",
    "            is_full_sentence = False\n",
    "\n",
    "        # construct an array where 1s represent the position of the target word\n",
    "        known_evidence = np.zeros(len(common_sentence_tokenized))\n",
    "        evidence_index = np.where(np.array(common_sentence_tokenized) == target)[0][0]\n",
    "        known_evidence[evidence_index] = 1\n",
    "\n",
    "        extracted_data = [known_evidence, common_sentence, correct_word, foil_word, is_full_sentence]\n",
    "        extracted_sentences_data.append(extracted_data)\n",
    "\n",
    "    print(f\"The number of pairs with different number of words: {dif_lengths_counter}\")\n",
    "    print(f\"Number of processed senteces: {len(extracted_sentences_data)}/{len(good_bad_sentences)}\")\n",
    "\n",
    "    return extracted_sentences_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64ba77bd-99f0-4aea-91e6-87633b8b01d0",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "### 2. Compute saliency maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "fa18c957-60e5-404b-8ed5-c693e1e58564",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def match_evidence_with_tokenization(space_tokenization, gpt_tokenization, known_evidence):\n",
    "    # this function adds 0s or 1s in the known evidence to match the list of GPT tokens\n",
    "\n",
    "    space_tokenization_index = 0\n",
    "    updated_known_evidence = []\n",
    "    accumulated_string = gpt_tokenization[0]\n",
    "\n",
    "    for gpt_tokenization_index, gpt_token in enumerate(gpt_tokenization[1:]):\n",
    "        if accumulated_string == space_tokenization[space_tokenization_index]:\n",
    "            updated_known_evidence.append(known_evidence[space_tokenization_index])\n",
    "            accumulated_string = gpt_tokenization[gpt_tokenization_index + 1]\n",
    "            space_tokenization_index += 1\n",
    "\n",
    "        else:\n",
    "            accumulated_string += gpt_token\n",
    "            updated_known_evidence.append(known_evidence[space_tokenization_index])\n",
    "\n",
    "    updated_known_evidence.append(known_evidence[space_tokenization_index])\n",
    "\n",
    "    return updated_known_evidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70868bfd-bfb3-40bf-bd6d-47b909cc418c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def compute_alignment_metrics(sentence_data, contrastive_method):\n",
    "    known_evidence = sentence_data[0]\n",
    "    sentence = sentence_data[1]\n",
    "    correct_word = sentence_data[2]\n",
    "    foil_word = sentence_data[3]\n",
    "\n",
    "    saliency_map = contrastive_method(sentence, correct_word, foil_word)\n",
    "    extracted_words = [explanation[0].strip() for explanation in saliency_map]\n",
    "    explanation = [explanation[1] for explanation in saliency_map]\n",
    "\n",
    "    if math.isnan(saliency_map[0][1]):\n",
    "        return None\n",
    "\n",
    "    known_evidence = match_evidence_with_tokenization(sentence.split(\" \"), extracted_words, known_evidence)\n",
    "    mean_dot_product = compute_mean_dot_product([explanation], [known_evidence])\n",
    "    mean_probes_needed = compute_mean_probes_needed([explanation], [known_evidence])\n",
    "    mean_reciprocal_rank = compute_mean_reciprocal_rank([explanation], [known_evidence])\n",
    "\n",
    "    return saliency_map, mean_dot_product, mean_probes_needed, mean_reciprocal_rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05128905-b7c1-4776-a3b4-10c0c919e084",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_contrastive_method(index, contrastive_explanations):\n",
    "    if index == 0:\n",
    "        return \"gradient norm\", contrastive_explanations.get_contrastive_gradient_norm\n",
    "    elif index == 1:\n",
    "        return \"input x gradient\", contrastive_explanations.get_contrastive_input_x_gradient\n",
    "    else:\n",
    "        return \"input erasure\", contrastive_explanations.get_input_erasure"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "978ee114-635a-4262-9bbc-620401a86984",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "### 3. Extract data for form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c114e5f2-8a84-458c-851a-e1f6023e88fc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def extract_form_data(extracted_sentences_data, phenomenon, contrastive_explanations, number_of_examples=8):\n",
    "    index = 0\n",
    "    form_data = []\n",
    "    random.shuffle(extracted_sentences_data)\n",
    "\n",
    "    for sentence_data in extracted_sentences_data:\n",
    "        contrastive_function_name, contrastive_function = get_contrastive_method(index, contrastive_explanations)\n",
    "        saliency_map, _, _, _ = compute_alignment_metrics(sentence_data, contrastive_function)\n",
    "\n",
    "        if contrastive_function_name is None or saliency_map is None:\n",
    "            continue\n",
    "\n",
    "        form_data.append(\n",
    "            {\n",
    "                \"phenomenon\": phenomenon,\n",
    "                \"input sentence\": sentence_data[1],\n",
    "                \"correct word\": sentence_data[2],\n",
    "                \"wrong word\": sentence_data[3],\n",
    "                \"contrastive function\": contrastive_function_name,\n",
    "                \"explanation\": saliency_map,\n",
    "            }\n",
    "        )\n",
    "\n",
    "        index += 1\n",
    "\n",
    "        if index == number_of_examples:\n",
    "            break\n",
    "\n",
    "    return form_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6dff82b-aa52-4a16-9e0e-0b7d2b05b8de",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "global_form_data = []\n",
    "\n",
    "for phenomenon, datasets_for_phenomenon in zip(phenomena, datasets):\n",
    "    data = get_data(datasets_for_phenomenon)\n",
    "    extracted_sentences_data = preprocess_data(data, phenomenon)\n",
    "    global_form_data += extract_form_data(extracted_sentences_data, phenomenon, contrastive_explanations)\n",
    "\n",
    "with open('./data/form_data.json', 'w') as f:\n",
    "    json.dump(global_form_data, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "102e003d-b1b9-4316-a8fe-c7c4d34ca4a7",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "### 4. Compute alignment metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "e596cb08-46ac-4575-b2d7-c403d03e1651",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset_names = [\n",
    "    \"anaphor_gender_agreement\",\n",
    "    \"anaphor_number_agreement\",\n",
    "    \"animate_subject_passive\",\n",
    "    \"determiner_noun_agreement_1\",\n",
    "    \"determiner_noun_agreement_irregular_1\",\n",
    "    \"determiner_noun_agreement_with_adjective_1\",\n",
    "    \"determiner_noun_agreement_with_adj_irregular_1\",\n",
    "    \"npi_present_1\",\n",
    "    \"distractor_agreement_relational_noun\",\n",
    "    \"irregular_plural_subject_verb_agreement_1\",\n",
    "    \"regular_plural_subject_verb_agreement_1\",\n",
    "]\n",
    "\n",
    "phenomenon_names = [\n",
    "    \"anaphor agreement\",\n",
    "    \"anaphor agreement\",\n",
    "    \"argument structure\",\n",
    "    \"determiner noun agreement\",\n",
    "    \"determiner noun agreement\",\n",
    "    \"determiner noun agreement\",\n",
    "    \"determiner noun agreement\",\n",
    "    \"npi licensing\",\n",
    "    \"subject verb agreement\",\n",
    "    \"subject verb agreement\",\n",
    "    \"subject verb agreement\",\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "f74cb4af-7c36-4bb5-9aff-8219c8e096c6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def preprocess_all_data(phenomenon_names, dataset_names):\n",
    "    all_extracted_sentences_data = []\n",
    "    number_of_invalid_examples = []\n",
    "\n",
    "    for phenomenon_name, dataset_name in zip(phenomenon_names, dataset_names):\n",
    "        data = get_data([dataset_name])\n",
    "        extracted_sentences_data = preprocess_data(data, phenomenon_name)\n",
    "        all_extracted_sentences_data.extend(extracted_sentences_data)\n",
    "        number_of_invalid_examples.append(data.shape[0] - len(extracted_sentences_data))\n",
    "\n",
    "    return all_extracted_sentences_data, number_of_invalid_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "0a79105f-85aa-45f3-b0d7-3bbc336cba28",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_metric_entire_dataset(all_extracted_sentences_data, contrastive_method, contrastive_method_name):\n",
    "    explanations = []\n",
    "    known_evidences = []\n",
    "\n",
    "    for sentence_data in tqdm(all_extracted_sentences_data):\n",
    "        known_evidence = sentence_data[0]\n",
    "        sentence = sentence_data[1]\n",
    "        correct_word = sentence_data[2]\n",
    "        foil_word = sentence_data[3]\n",
    "\n",
    "        saliency_map = contrastive_method(sentence, correct_word, foil_word)\n",
    "        extracted_words = [explanation[0].strip() for explanation in saliency_map]\n",
    "        explanation = [explanation[1] for explanation in saliency_map]\n",
    "\n",
    "        if math.isnan(saliency_map[0][1]):\n",
    "            continue\n",
    "\n",
    "        known_evidence = match_evidence_with_tokenization(sentence.split(\" \"), extracted_words, known_evidence)\n",
    "\n",
    "        explanations.append(explanation)\n",
    "        known_evidences.append(known_evidence)\n",
    "\n",
    "    print(f\"Number of sentences considered in the end by {contrastive_method_name}: {len(explanations)}\")\n",
    "    mean_dot_product = compute_mean_dot_product(explanations, known_evidences)\n",
    "    mean_probes_needed = compute_mean_probes_needed(explanations, known_evidences)\n",
    "    mean_reciprocal_rank = compute_mean_reciprocal_rank(explanations, known_evidences)\n",
    "\n",
    "    return mean_dot_product, mean_probes_needed, mean_reciprocal_rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "772890c4-a7ed-41a7-95e9-691f7be37046",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_metrics_entire_dataset(all_extracted_sentences_datam, contrastive_explanations):\n",
    "    overall_metrics = {}\n",
    "\n",
    "    for contrastive_method_index in range(3):\n",
    "        contrastive_method_name, contrastive_method = get_contrastive_method(contrastive_method_index, contrastive_explanations)\n",
    "        mean_dot_product, mean_probes_needed, mean_reciprocal_rank = get_metric_entire_dataset(all_extracted_sentences_data, contrastive_method, contrastive_method_name)\n",
    "\n",
    "        overall_metrics[f\"mean dot product - {contrastive_method_name}\"] = mean_dot_product\n",
    "        overall_metrics[f\"mean probes needed - {contrastive_method_name}\"] = mean_probes_needed\n",
    "        overall_metrics[f\"mean reciprocal rank - {contrastive_method_name}\"] = mean_reciprocal_rank\n",
    "\n",
    "        with open('./data/metrics_contrastive_explanations.json', 'w') as f:\n",
    "            json.dump(overall_metrics, f)\n",
    "\n",
    "    return overall_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "65b4d036-7bac-49fb-89e5-57454d3ea1d9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:05<00:00, 179.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of pairs with different number of words: 12\n",
      "Number of processed senteces: 1000/1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:04<00:00, 208.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of pairs with different number of words: 0\n",
      "Number of processed senteces: 983/1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:04<00:00, 208.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of pairs with different number of words: 0\n",
      "Number of processed senteces: 945/1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:04<00:00, 210.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of pairs with different number of words: 0\n",
      "Number of processed senteces: 964/1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:04<00:00, 215.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of pairs with different number of words: 0\n",
      "Number of processed senteces: 931/1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:00<00:00, 54087.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of pairs with different number of words: 0\n",
      "Number of processed senteces: 1000/1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:08<00:00, 116.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of pairs with different number of words: 0\n",
      "Number of processed senteces: 865/1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:07<00:00, 128.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of pairs with different number of words: 0\n",
      "Number of processed senteces: 854/1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:08<00:00, 124.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of pairs with different number of words: 0\n",
      "Number of processed senteces: 853/1000\n",
      "Total number of invalid examples per phenomenon: [0, 17, 55, 36, 69, 0, 135, 146, 147]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8395/8395 [17:00<00:00,  8.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sentences considered in the end by gradient norm: 6204\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8395/8395 [16:56<00:00,  8.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sentences considered in the end by input x gradient: 6204\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8395/8395 [31:06<00:00,  4.50it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sentences considered in the end by input erasure: 6204\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "all_extracted_sentences_data, number_of_invalid_examples = preprocess_all_data(phenomenon_names, dataset_names)\n",
    "print(f\"Total number of invalid examples per phenomenon: {number_of_invalid_examples}\")\n",
    "overall_metrics = get_metrics_entire_dataset(all_extracted_sentences_data, contrastive_explanations)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41340878-51a5-4bbe-adf5-4666df483eac",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "### 5. Plot explanation (saliency map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "542c148b-3e31-4659-9680-6e4a28bebaa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_explanation(words, scores, name=\"explanation\"):\n",
    "    fig, ax = plt.subplots(1, 1)\n",
    "    fig.set_size_inches(1.2 * len(scores), 0.5)\n",
    "\n",
    "    rounded_scores = [round(score, 4) for score in scores]\n",
    "    xticks_positions = np.array(range(len(rounded_scores))) + 0.5\n",
    "\n",
    "    sns.heatmap(data=np.array([scores]), annot=np.array([words]), fmt=\"\", cmap='coolwarm', cbar=False, ax=ax)\n",
    "    ax.set_xticks(ticks=xticks_positions, labels=np.array(rounded_scores))\n",
    "    ax.set_yticks([])\n",
    "\n",
    "    plt.show()\n",
    "    fig.savefig(f\"./results/{name}.png\", bbox_inches='tight') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88209e20-8ae5-4bdb-945a-e4b7dc5c4d3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = [info[0] for info in global_form_data[10][\"explanation\"]]\n",
    "scores = [info[1] for info in global_form_data[10][\"explanation\"]]\n",
    "plot_explanation(words, scores)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
