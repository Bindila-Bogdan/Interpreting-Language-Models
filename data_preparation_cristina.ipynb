{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "363a078c-6808-4bf5-ad66-750c2047b5b4",
   "metadata": {
    "tags": [],
    "user_expressions": []
   },
   "source": [
    "### 0. Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "62614185-9ef7-461f-a03f-3722799697a9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: spacy in /home/jovyan/.local/lib/python3.8/site-packages (3.5.4)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /home/jovyan/.local/lib/python3.8/site-packages (from spacy) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /home/jovyan/.local/lib/python3.8/site-packages (from spacy) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /home/jovyan/.local/lib/python3.8/site-packages (from spacy) (1.0.10)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /home/jovyan/.local/lib/python3.8/site-packages (from spacy) (2.0.8)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /home/jovyan/.local/lib/python3.8/site-packages (from spacy) (3.0.9)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /home/jovyan/.local/lib/python3.8/site-packages (from spacy) (8.1.12)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /home/jovyan/.local/lib/python3.8/site-packages (from spacy) (0.10.1)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /home/jovyan/.local/lib/python3.8/site-packages (from spacy) (2.4.8)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /home/jovyan/.local/lib/python3.8/site-packages (from spacy) (2.0.10)\n",
      "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /home/jovyan/.local/lib/python3.8/site-packages (from spacy) (0.9.4)\n",
      "Requirement already satisfied: pathy>=0.10.0 in /home/jovyan/.local/lib/python3.8/site-packages (from spacy) (0.11.0)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /home/jovyan/.local/lib/python3.8/site-packages (from spacy) (6.4.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /home/jovyan/.local/lib/python3.8/site-packages (from spacy) (4.66.4)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.8/dist-packages (from spacy) (1.23.1)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /home/jovyan/.local/lib/python3.8/site-packages (from spacy) (2.32.3)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /home/jovyan/.local/lib/python3.8/site-packages (from spacy) (1.10.17)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.8/dist-packages (from spacy) (3.1.3)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from spacy) (69.5.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from spacy) (24.0)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /home/jovyan/.local/lib/python3.8/site-packages (from spacy) (3.4.0)\n",
      "Requirement already satisfied: language-data>=1.2 in /home/jovyan/.local/lib/python3.8/site-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.2.0)\n",
      "Requirement already satisfied: pathlib-abc==0.1.1 in /home/jovyan/.local/lib/python3.8/site-packages (from pathy>=0.10.0->spacy) (0.1.1)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.8/dist-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy) (4.11.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.8)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/lib/python3/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (1.25.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2019.11.28)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /home/jovyan/.local/lib/python3.8/site-packages (from thinc<8.2.0,>=8.1.8->spacy) (0.7.11)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /home/jovyan/.local/lib/python3.8/site-packages (from thinc<8.2.0,>=8.1.8->spacy) (0.1.5)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.8/dist-packages (from typer<0.10.0,>=0.3.0->spacy) (8.1.7)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.8/dist-packages (from jinja2->spacy) (2.1.5)\n",
      "Requirement already satisfied: marisa-trie>=0.7.7 in /home/jovyan/.local/lib/python3.8/site-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pip in /home/jovyan/.local/lib/python3.8/site-packages (24.0)\n",
      "Collecting pip\n",
      "  Downloading pip-24.1.1-py3-none-any.whl.metadata (3.6 kB)\n",
      "Downloading pip-24.1.1-py3-none-any.whl (1.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: pip\n",
      "  Attempting uninstall: pip\n",
      "    Found existing installation: pip 24.0\n",
      "    Uninstalling pip-24.0:\n",
      "      Successfully uninstalled pip-24.0\n",
      "\u001b[33m  WARNING: The scripts pip, pip3 and pip3.8 are installed in '/home/jovyan/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "orange3 3.36.2 requires scikit-learn!=1.2.*,<1.4,>=1.1.0, but you have scikit-learn 1.2.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed pip-24.1.1\n"
     ]
    }
   ],
   "source": [
    "# These two commands are needed\n",
    "# !pip install transformers\n",
    "# !pip install accelerate\n",
    "!pip install spacy\n",
    "# !python -m spacy download en\n",
    "# !pip uninstall spacy -y\n",
    "# !python -m pip install spacy==2.1.0.\n",
    "# !pip install neuralcoref"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7336fa36-a5dc-43d9-976e-427a9e8c4e9b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1a29510a-b6f4-492a-b028-0776461b1199",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "/usr/lib/x86_64-linux-gnu/libcuda.so.1: file too short",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# import libraries\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mre\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mspacy\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# import neuralcoref\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/spacy/__init__.py:6\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msys\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# set library-specific custom warning handling before doing anything else\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01merrors\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m setup_default_warnings\n\u001b[1;32m      8\u001b[0m setup_default_warnings()  \u001b[38;5;66;03m# noqa: E402\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# These are imported as part of the API\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/spacy/errors.py:2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mwarnings\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Literal\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mErrorsWithCodes\u001b[39;00m(\u001b[38;5;28mtype\u001b[39m):\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getattribute__\u001b[39m(\u001b[38;5;28mself\u001b[39m, code):\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/spacy/compat.py:3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"Helpers for Python and platform compatibility.\"\"\"\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msys\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mthinc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m copy_array\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mcPickle\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpickle\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/thinc/__init__.py:5\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mabout\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m __version__\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconfig\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m registry\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# fmt: off\u001b[39;00m\n\u001b[1;32m      8\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mregistry\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__version__\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     11\u001b[0m ]\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/thinc/config.py:5\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mconfection\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mconfection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m VARIABLE_RE, Config, ConfigValidationError, Promise\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtypes\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Decorator\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mregistry\u001b[39;00m(confection\u001b[38;5;241m.\u001b[39mregistry):\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;66;03m# fmt: off\u001b[39;00m\n\u001b[1;32m     10\u001b[0m     optimizers: Decorator \u001b[38;5;241m=\u001b[39m catalogue\u001b[38;5;241m.\u001b[39mcreate(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthinc\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moptimizers\u001b[39m\u001b[38;5;124m\"\u001b[39m, entry_points\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/thinc/types.py:25\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m      5\u001b[0m     Any,\n\u001b[1;32m      6\u001b[0m     Callable,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     20\u001b[0m     overload,\n\u001b[1;32m     21\u001b[0m )\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m\n\u001b[0;32m---> 25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m cupy, has_cupy\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_cupy:\n\u001b[1;32m     28\u001b[0m     get_array_module \u001b[38;5;241m=\u001b[39m cupy\u001b[38;5;241m.\u001b[39mget_array_module\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/thinc/compat.py:66\u001b[0m\n\u001b[1;32m     62\u001b[0m     has_tensorflow_gpu \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:  \u001b[38;5;66;03m# pragma: no cover\u001b[39;00m\n\u001b[0;32m---> 66\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmxnet\u001b[39;00m\n\u001b[1;32m     68\u001b[0m     has_mxnet \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m     69\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:  \u001b[38;5;66;03m# pragma: no cover\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/mxnet/__init__.py:23\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#!/usr/bin/env python\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Licensed to the Apache Software Foundation (ASF) under one\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     19\u001b[0m \n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# coding: utf-8\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;124;03m\"\"\"MXNet: a concise, fast and flexible framework for deep learning.\"\"\"\u001b[39;00m\n\u001b[0;32m---> 23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcontext\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Context, current_context, cpu, gpu, cpu_pinned\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m engine, error\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MXNetError\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/mxnet/context.py:23\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mwarnings\u001b[39;00m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mctypes\u001b[39;00m\n\u001b[0;32m---> 23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m classproperty, with_metaclass, _MXClassPropertyMetaClass\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _LIB\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m check_call\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/mxnet/base.py:356\u001b[0m\n\u001b[1;32m    354\u001b[0m __version__ \u001b[38;5;241m=\u001b[39m libinfo\u001b[38;5;241m.\u001b[39m__version__\n\u001b[1;32m    355\u001b[0m \u001b[38;5;66;03m# library instance of mxnet\u001b[39;00m\n\u001b[0;32m--> 356\u001b[0m _LIB \u001b[38;5;241m=\u001b[39m \u001b[43m_load_lib\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    358\u001b[0m \u001b[38;5;66;03m# type definitions\u001b[39;00m\n\u001b[1;32m    359\u001b[0m mx_int \u001b[38;5;241m=\u001b[39m ctypes\u001b[38;5;241m.\u001b[39mc_int\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/mxnet/base.py:347\u001b[0m, in \u001b[0;36m_load_lib\u001b[0;34m()\u001b[0m\n\u001b[1;32m    345\u001b[0m     lib \u001b[38;5;241m=\u001b[39m ctypes\u001b[38;5;241m.\u001b[39mCDLL(lib_path[\u001b[38;5;241m0\u001b[39m], winmode\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0x00000008\u001b[39m)\n\u001b[1;32m    346\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 347\u001b[0m     lib \u001b[38;5;241m=\u001b[39m \u001b[43mctypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCDLL\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlib_path\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mRTLD_LOCAL\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    348\u001b[0m \u001b[38;5;66;03m# DMatrix functions\u001b[39;00m\n\u001b[1;32m    349\u001b[0m lib\u001b[38;5;241m.\u001b[39mMXGetLastError\u001b[38;5;241m.\u001b[39mrestype \u001b[38;5;241m=\u001b[39m ctypes\u001b[38;5;241m.\u001b[39mc_char_p\n",
      "File \u001b[0;32m/usr/lib/python3.8/ctypes/__init__.py:373\u001b[0m, in \u001b[0;36mCDLL.__init__\u001b[0;34m(self, name, mode, handle, use_errno, use_last_error, winmode)\u001b[0m\n\u001b[1;32m    370\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_FuncPtr \u001b[38;5;241m=\u001b[39m _FuncPtr\n\u001b[1;32m    372\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m handle \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 373\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_handle \u001b[38;5;241m=\u001b[39m \u001b[43m_dlopen\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    374\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    375\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_handle \u001b[38;5;241m=\u001b[39m handle\n",
      "\u001b[0;31mOSError\u001b[0m: /usr/lib/x86_64-linux-gnu/libcuda.so.1: file too short"
     ]
    }
   ],
   "source": [
    "# import libraries\n",
    "import re\n",
    "import spacy\n",
    "import torch\n",
    "# import neuralcoref\n",
    "import numpy as np\n",
    "import transformers\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from functools import reduce\n",
    "\n",
    "# import local scripts\n",
    "from data_loader import DatasetLoader\n",
    "from alignment_metrics import *\n",
    "from gpt2_model import GPT2Model\n",
    "\n",
    "# produce repeatable results\n",
    "np.random.seed(seed=42)\n",
    "transformers.set_seed(42)\n",
    "\n",
    "# enable CUDNN deterministic mode\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c4173ef-0513-4bb0-a170-2129d03d67e2",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 1. Load the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3b8d313-2265-4d12-8d46-b8f961cbc7e7",
   "metadata": {},
   "source": [
    "Below are the names of the datasets used by the authors to check if contrastive explanations identify linguistically appropriate evidence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bb67dff-7496-415c-b1b2-978768144041",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "anaphor_agreement_datasets = [\"anaphor_gender_agreement\", \"anaphor_number_agreement\"]\n",
    "\n",
    "# not sure about the one below\n",
    "argument_structure_datasets = [\"animate_subject_passive\"]\n",
    "\n",
    "determiner_noun_agreement_datasets = [\n",
    "    \"determiner_noun_agreement_1\",\n",
    "    \"determiner_noun_agreement_2\",\n",
    "    \"determiner_noun_agreement_irregular_1\",\n",
    "    \"determiner_noun_agreement_irregular_2\",\n",
    "    \"determiner_noun_agreement_with_adj_2\",\n",
    "    \"determiner_noun_agreement_with_adj_irregular_1\",\n",
    "    \"determiner_noun_agreement_with_adj_irregular_2\",\n",
    "    \"determiner_noun_agreement_with_adjective_1\",\n",
    "    \"determiner_noun_agreement_with_adj_irregular_1\",\n",
    "    \"determiner_noun_agreement_with_adj_irregular_2\"\n",
    "]\n",
    "\n",
    "npi_licesing = [\n",
    "    \"npi_present_1\",\n",
    "    \"npi_present_2\"\n",
    "]\n",
    "\n",
    "subject_verb_agreement = [\n",
    "    \"distractor_agreement_relational_noun\"\n",
    "    \"irregular_plural_subject_verb_agreement_1\",\n",
    "    \"irregular_plural_subject_verb_agreement_2\",\n",
    "    \"regular_plural_subject_verb_agreement_1\",\n",
    "    \"regular_plural_subject_verb_agreement_2\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89280074-a98f-410b-9bb6-db1882ec86da",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_loader = DatasetLoader()\n",
    "data = pd.DataFrame(data_loader.load_data(argument_structure_datasets[0])[\"train\"])\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b94351e1-83f3-40ae-a8c0-82dc2e0d2ed8",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 2. Prepare anaphor agreement data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09668026-f097-478e-9bf3-399618d4b2a5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_main_verb(sentence):\n",
    "    doc = nlp(sentence)\n",
    "    main_verbs = []\n",
    "\n",
    "    for token in doc:\n",
    "        if token.pos_ == \"VERB\":\n",
    "            main_verbs.append(token.text)\n",
    "\n",
    "    assert len(main_verbs) == 1\n",
    "\n",
    "    return main_verbs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cd0f7f0-c339-4d5f-abfd-96be969530e4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "extracted_sentences_data = []\n",
    "good_bad_sentences = data[[\"sentence_good\", \"sentence_bad\"]].to_numpy()\n",
    "dif_lengths = 0\n",
    "\n",
    "for good_sentence, bad_sentence in tqdm(good_bad_sentences):\n",
    "    # remove characters that are not letters or apostrophes and replace contractions\n",
    "    good_sentence_cleaned = re.sub(r\"[^\\w\\s']\", \"\", good_sentence).replace(\"n't\", \" not\")\n",
    "    bad_sentence_cleaned = re.sub(r\"[^\\w\\s']\", \"\", bad_sentence).replace(\"n't\", \" not\")\n",
    "\n",
    "    # tokenize the text\n",
    "    good_sentence_tokenized = good_sentence_cleaned.split(\" \")\n",
    "    bad_sentence_tokenized = bad_sentence_cleaned.split(\" \")\n",
    "\n",
    "    # consider only pairs of sentences with the same number of words\n",
    "    number_considered_words = min(len(good_sentence_tokenized), len(bad_sentence_tokenized))\n",
    "    if len(good_sentence_tokenized) != len(bad_sentence_tokenized):\n",
    "        dif_lengths += 1\n",
    "\n",
    "    good_sentence_tokenized = good_sentence_tokenized[:number_considered_words]\n",
    "    bad_sentence_tokenized = bad_sentence_tokenized[:number_considered_words]\n",
    "\n",
    "    # get the main verb of the sentence\n",
    "    main_verb = get_main_verb(good_sentence)\n",
    "\n",
    "    # get the common part of the two sentences (until the first different word)\n",
    "    same_tokens = np.array(good_sentence_tokenized) == np.array(bad_sentence_tokenized)\n",
    "    index_first_diff_token = np.where(same_tokens == False)[0][0]\n",
    "\n",
    "    common_sentence_tokenized = good_sentence_tokenized[:index_first_diff_token]\n",
    "    common_sentence = \" \".join(common_sentence_tokenized)\n",
    "\n",
    "    # construct an array where 1s represent the position of the target word\n",
    "    known_evidence = np.zeros(len(common_sentence_tokenized))\n",
    "    evidence_index = np.where(np.array(common_sentence_tokenized) == main_verb)[0][0]\n",
    "    known_evidence[evidence_index] = 1\n",
    "\n",
    "    # get the correct and foil words\n",
    "    correct_word = good_sentence_tokenized[index_first_diff_token]\n",
    "    foil_word = bad_sentence_tokenized[index_first_diff_token]\n",
    "\n",
    "    extracted_sentences_data.append([known_evidence, common_sentence, correct_word, foil_word])\n",
    "\n",
    "print(f\"The number of pairs with different number of words: {dif_lengths}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "172bda0c-0298-48fc-83d4-fd09b24049f3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "gpt2model = GPT2Model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa18c957-60e5-404b-8ed5-c693e1e58564",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def match_evidence_with_tokenization(space_tokenization, gpt_tokenization, known_evidence):\n",
    "    # this function adds 0s or 1s in the known evidence to match the gpt tokens\n",
    "\n",
    "    space_tokenization_index = 0\n",
    "    updated_known_evidence = []\n",
    "    accumulated_string = gpt_tokenization[0]\n",
    "\n",
    "    for gpt_tokenization_index, gpt_token in enumerate(gpt_tokenization[1:]):\n",
    "        if accumulated_string == space_tokenization[space_tokenization_index]:\n",
    "            updated_known_evidence.append(known_evidence[space_tokenization_index])\n",
    "            gpt_tokenization_index += 1\n",
    "            accumulated_string = gpt_tokenization[gpt_tokenization_index]\n",
    "            space_tokenization_index += 1\n",
    "\n",
    "        else:\n",
    "            accumulated_string += gpt_token\n",
    "            updated_known_evidence.append(known_evidence[space_tokenization_index])\n",
    "\n",
    "    updated_known_evidence.append(known_evidence[space_tokenization_index])\n",
    "\n",
    "    return updated_known_evidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fe1695d-0bef-456c-9702-1bfaf56f9bf5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "different_tokenization = 0\n",
    "gradient_norm_dot_product = []\n",
    "gradient_norm_probes_needed = []\n",
    "\n",
    "\n",
    "for sentence_data in tqdm(extracted_sentences_data[:10]):\n",
    "    known_evidence = sentence_data[0]\n",
    "    sentence = sentence_data[1]\n",
    "    correct_word = sentence_data[2]\n",
    "    foil_word = sentence_data[3]\n",
    "\n",
    "    saliency_map = gpt2model.get_contrastive_gradient_norm(sentence, correct_word, foil_word)\n",
    "    extracted_words = [explanation[0].strip() for explanation in saliency_map]\n",
    "    explanation = [explanation[1] for explanation in saliency_map]\n",
    "\n",
    "    if sentence.split(\" \") != extracted_words:\n",
    "        known_evidence = match_evidence_with_tokenization(sentence.split(\" \"), extracted_words, known_evidence)\n",
    "\n",
    "    updated_known_evidence = match_evidence_with_tokenization(sentence.split(\" \"), extracted_words, known_evidence)\n",
    "    gradient_norm_dot_product.append(compute_mean_dot_product([explanation], [updated_known_evidence]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8492186b-9d0a-43c2-976b-81d40762837a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "np.array(gradient_norm_dot_product).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0040ac5-e610-4221-b40d-3716e4b25892",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_loader = DatasetLoader()\n",
    "data = data_loader.load_data(determiner_noun_agreement_datasets[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcc01c3e-c26a-4637-9ca6-b4c7ce1ffb37",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(data[\"train\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eaedd71-2140-40aa-81a1-7921d200ae4d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def extract_determiner_from_target_noun(text, target_noun):\n",
    "    doc = nlp(text)\n",
    "\n",
    "    for token in doc:\n",
    "        if token.text.lower() == target_noun.lower() and token.pos_ == \"NOUN\":\n",
    "            for child in token.children:\n",
    "                if child.dep_ == \"det\":\n",
    "                    return child.text\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a4c33a2-2ac1-4b16-9549-ee759da74b60",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "text = \"Those people sound like this art gallery.\"\n",
    "target_noun = \"gallery\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fb88f07-7fcb-47d6-8c33-7c2a3afe9332",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "extract_determiner_from_target_noun(text, target_noun)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98940723-2c57-4697-9454-3a3471055c88",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "str(pd.DataFrame(data[\"train\"]).sentence_good)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f14fc7f-2e0b-4c81-8cb1-75d105f00172",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_loader = DatasetLoader()\n",
    "data = data_loader.load_data(npi_licesing[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb731c3d-2b05-47c5-a61b-79994ea21f02",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(data[\"train\"])['sentence_good'].apply(lambda x: str(x).startswith(\"Even\")).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8495324c-ecfa-45fb-94cc-e2e59bb17eeb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# so for NPI the only underlined word will be \"Even\",\n",
    "# which is the first one for all the examples in our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f7aa58a-7a8b-4466-8952-6d0d9fc3d259",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_loader = DatasetLoader()\n",
    "data = data_loader.load_data(subject_verb_agreement[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94318341-7986-422f-bd74-f7fe56837501",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(data[\"train\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "678ab88d-2c9b-4f6f-9a10-0901dfea0f0e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def extract_subject_from_target_verb(text, target_verb):\n",
    "    doc = nlp(text)\n",
    "    for token in doc:\n",
    "        if token.text.lower() == target_verb.lower() and (token.pos_ == \"VERB\" or token.pos_ == \"AUX\"):\n",
    "            for child in token.children:\n",
    "                if child.dep_ in {\"nsubj\", \"nsubjpass\"}:\n",
    "                    return child.text\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6839d12b-daa7-4609-9513-d66bd7e37db4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_verb_from_sentence(text):\n",
    "    doc = nlp(text)\n",
    "    for token in doc:\n",
    "        if token.text.lower() and token.pos_ == \"VERB\":\n",
    "            return token.text\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "464c8246-5119-4e1e-bff6-c99947347f37",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "text = \"The alumni hate all ladies.\"\n",
    "extract_subject_from_target_verb(text, get_verb_from_sentence(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "90c2fe0b-dd6e-41aa-8cad-7066ff4c2d67",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'DatasetLoader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m data_loader \u001b[38;5;241m=\u001b[39m \u001b[43mDatasetLoader\u001b[49m()\n\u001b[1;32m      2\u001b[0m data \u001b[38;5;241m=\u001b[39m data_loader\u001b[38;5;241m.\u001b[39mload_data(anaphor_agreement_datasets[\u001b[38;5;241m0\u001b[39m])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'DatasetLoader' is not defined"
     ]
    }
   ],
   "source": [
    "data_loader = DatasetLoader()\n",
    "data = data_loader.load_data(anaphor_agreement_datasets[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2e91ec53-7145-4166-91b2-d6f6048f7ab8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mpd\u001b[49m\u001b[38;5;241m.\u001b[39mDataFrame(data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "pd.DataFrame(data[\"train\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4fe270d5-b986-46d7-bf21-2d4b8b7cb76e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nlp' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 25\u001b[0m\n\u001b[1;32m     23\u001b[0m text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMany teenagers were helping themselves.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     24\u001b[0m target_reflexive \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthemselves\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 25\u001b[0m antecedent \u001b[38;5;241m=\u001b[39m \u001b[43mextract_reflexive_antecedent\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_reflexive\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe antecedent for \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtarget_reflexive\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m is: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mantecedent\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[6], line 3\u001b[0m, in \u001b[0;36mextract_reflexive_antecedent\u001b[0;34m(text, target_reflexive)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mextract_reflexive_antecedent\u001b[39m(text, target_reflexive):\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;66;03m# Process the text with spaCy\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m     doc \u001b[38;5;241m=\u001b[39m \u001b[43mnlp\u001b[49m(text)\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;66;03m# Find the target reflexive pronoun in the document\u001b[39;00m\n\u001b[1;32m      6\u001b[0m     target_token \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'nlp' is not defined"
     ]
    }
   ],
   "source": [
    "def extract_reflexive_antecedent(text, target_reflexive):\n",
    "    doc = nlp(text)\n",
    "\n",
    "    target_token = None\n",
    "    for token in doc:\n",
    "        if token.text.lower() == target_reflexive.lower() and (token.dep_ == \"pobj\" or token.dep_ == \"dobj\") :  # Reflexive pronouns usually have 'pobj' dependency\n",
    "            target_token = token\n",
    "            break\n",
    "\n",
    "    if not target_token:\n",
    "        return None\n",
    "\n",
    "    for token in target_token.head.lefts:\n",
    "        if token.dep_ in {\"nsubj\", \"nsubjpass\"} and token.pos_ == \"NOUN\":\n",
    "            return token.text\n",
    "    \n",
    "    return None\n",
    "\n",
    "text = \"Many teenagers were helping themselves.\"\n",
    "target_reflexive = \"themselves\"\n",
    "antecedent = extract_reflexive_antecedent(text, target_reflexive)\n",
    "\n",
    "print(f\"The antecedent for '{target_reflexive}' is: {antecedent}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47fdb8bf-8e6b-4716-b6e1-cc86a5bcbc57",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
